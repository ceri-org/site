---
title: AI Safety Resources
layout: page
---

Artificial intelligence is likely to be perhaps the most transformative technology of this century. Experts predict that human-level AI is fairly likely by mid-century, though some have much shorter or longer time lines, and uncertainty is obviously great. While potentially enormously beneficial, there are many challenges before we can deploy human-level or greater AI systems while being confident this will not have enormous unintended – and potentially catastrophic – consequences.

You can apply to work on a project relevant to understanding the effects of AI on existential risk at our [Summer Research Fellowship](https://camxrisk.org/fellowships/).

Opportunities in Cambridge relevant to AI alignemnt are regularly posted on our [mailing list](http://eepurl.com/hGPPkf).

Some resources that may be of interest:

## Quick introductions
- [Rob Miles (Youtube video)](https://www.youtube.com/watch?v=pYXy-A4siMw)
- [The case for taking AI seriously as a threat to humanity](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment) (Kelsey Piper, Vox)
- [Benefits and Risks of Artificial Intelligence](http://futureoflife.org/background/benefits-risks-of-artificial-intelligence/) (Future of Life Institute)

## More comprehensive introductions
- [An Introduction to the AI Alignment Landscape](https://www.alignmentforum.org/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view) (Neel Nanda, former EA Cambridge member, current researcher at Anthropic)
- [2022 AGI Safety Fundamentals Curriculum](https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/edit?usp=sharing) (curriculum designed by Richard Ngo, former DeepMind ML research engineer and currently on the OpenAI policy team)

## Newsletters & mailing lists
- [The Alignment Newsletter](https://rohinshah.com/alignment-newsletter/) (weekly updates on research on the AI alignment problem)

## Forums / research hubs
- [The Alignment Forum](https://www.alignmentforum.org/)

## Books
- The Alignment Problem (Brian Christian)
- Human Compatible (Stuart Russell)
- Superintelligence (Nick Bostrom)
